import os
import numpy as np
import tensorflow as tf
from matplotlib import pyplot as plt
from random import shuffle


#print os.listdir(TRAIN_DIR)


def decode_image(image_file_path, resize_func=None):
    images = []
    graph = tf.Graph()
    with graph.as_default():

        file_path = tf.placeholder(dtype=tf.string)
        #file_name = tf.placeholder(dtype=tf.string)

        files = tf.read_file(file_path)
        image = tf.image.decode_jpeg(files,channels=3) ##uint8
        try:
            image = tf.image.resize_images(image,[224,224],method=1)

        except:
            print 'resize fail'

    with tf.Session(graph=graph) as session:
        session.run(tf.global_variables_initializer())
        for i in range(len(image_file_path)):
            #labeled = session.run(label,feed_dict={file_name : image_file_names[i]})
            images.append(session.run(image, feed_dict={file_path: image_file_path[i]}))
            if (i + 1) % 1000 == 0:
                print('Images processed: ', i + 1)
                #print tf.type(images)
                #print tf.shape(images)

        session.close()
    #shuffle(images)
    return images


'''
def random_data(image_file_names,image_file_path):
    inputs = decode_image(image_file_path=image_file_path)
    randoms = {}
    for i in range(len(image_file_names)):
        file_name = image_file_names[i]
        try:
            spilt_name = file_name.spilt('.')
            print spilt_name
            if spilt_name[0] == 'cat':
                randoms[i] = [inputs[i],[0,1]]
            else:
                randoms[i] = [inputs[i],[1,0]]
        except:
            print 'one-hot label parse fail'
    shuffle(randoms)
    return randoms
'''

#train_images = decode_image(train_image_file_path)


'''
for i in range(3):
    plt.imshow(train_images[i])
    plt.show()
    '''
def seprete_data(train_images):
    train_data = []
    validation_data = []
    test_data = []
    num_images = len(train_images)
    for i in range(num_images):
        if i < 0.6*num_images:
            train_data.append(train_images[i])
        elif 0.6 * num_images <= i < 0.8 * num_images :
            validation_data.append(train_images[i])
        else:
            test_data.append(train_images[i])
    return train_data,validation_data,test_data


"""

plt.imshow(train_data[1])
plt.show()
plt.imshow(validation_data[1])
plt.show()
plt.imshow(test_data[1])
plt.show()
def module(self,rgb):
    VGG_MEAN = [103.939,116.779,123.68]
"""

def conv2d(input,filter):
    return tf.nn.conv2d(input,filter,strides=[1,1,1,1],padding='SAME')

def pool(input):
    return tf.nn.max_pool(input,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')

def weight_init(shape):
    initial = tf.truncated_normal(shape,stddev=0.1)
    return tf.Variable(initial)

def bias_init(shape):
    initial = tf.constant(0.1,shape=shape)
    return tf.Variable(initial)

def module(x):
    with tf.name_scope('reshape'):
        x_image = tf.cast(x,tf.float32)
        x_image = tf.reshape(x_image,[-1,224,224,3])
        print tf.shape(x_image)
        #tf.Print(x_image,[tf.shape(x_image)])

    with tf.name_scope('conv1'):
        W1_conv1 = weight_init([3,3,3,64])
        b1_conv1 = bias_init([64])
        h1_conv1 = tf.nn.relu(conv2d(x_image,W1_conv1) + b1_conv1)## translate to 224 X 224
        print h1_conv1

        W2_conv1 = weight_init([3,3,64,64])
        b2_conv1 = bias_init([64])
        h2_conv1 = tf.nn.relu(conv2d((h1_conv1,W2_conv1) + b2_conv1)) ## translate to 224 X 224

    with tf.name_scope('pool1'):
        pool1 = pool(h2_conv1) ## translate to 112 X 112

    with tf.name_scope('conv2'):
        W1_conv2 = weight_init([3,3,64,128])
        b1_conv2 = bias_init([128])
        h1_conv2 = tf.nn.relu(conv2d(pool1,W1_conv2) + b1_conv2) ## translate to 112 X 112

        W2_conv2 = weight_init([3,3,128,128])
        b2_conv2 = bias_init([128])
        h2_conv2 = tf.nn.relu(conv2d(h1_conv2,W2_conv2) + b2_conv2) ## translate to 112 X 112

    with tf.name_scope('pool2'):
        pool2 = pool(h2_conv2) ## translate to 56 X 56

    with tf.name_scope('fc1'):
        W1_fc1 = weight_init([56 * 56 * 128 , 4096])
        b1_fc1 = bias_init([4096])

        pool2_reshape = tf.reshape(pool2,[-1,56 * 56 * 128])
        h1_fc1 = tf.nn.relu(tf.matmul(pool2_reshape,W1_fc1) + b1_fc1)

    with tf.name_scope('fc2'):
        W1_fc2 = weight_init([4096,1024])
        b1_fc2 = bias_init([1024])
        h1_fc2 = tf.nn.relu(tf.matmul(h1_fc1,W1_fc2) + b1_fc2)

    with tf.name_scope('fc3'):
        W1_fc3 = weight_init([1024,256])
        b1_fc3 = bias_init([256])
        h1_fc3 = tf.nn.relu(tf.matmul(h1_fc2,W1_fc3) + b1_fc3)

    with tf.name_scope('fc4'):
        W1_fc4 = weight_init([256,2])
        b1_fc4 = bias_init([2])
        h1_fc4 = tf.nn.relu(tf.matmul(h1_fc3,W1_fc4) + b1_fc4)

    return h1_fc4

def main():
    TRAIN_DIR = '/Users/rubans/Desktop/cat_or_dog/train/'
    train_image_file_names = [i for i in os.listdir(TRAIN_DIR)][0:2000]
    train_image_file_path = [TRAIN_DIR + i for i in os.listdir(TRAIN_DIR)][0:2000]

    images = decode_image(train_image_file_path)
    train,validation,test = seprete_data(images)
    label = []
    for i in range(len(train_image_file_names)):
        splited = train_image_file_names[i].split('.')
        if splited[0] == 'cat':
            label.append([0,1])
        else:
            label.append([1,0])

    ## above is preprocess
    x = tf.placeholder(tf.float32,[None,224,224,3])

    y_ = tf.placeholder(tf.float32,[None,2])

    y_pred = module(x)

    with tf.name_scope('loss'):
        cross_enropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=y_pred)

    cross_enropy = tf.reduce_mean(cross_enropy)

    with tf.name_scope('accrancy'):
        corrects = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_,1))
        corrects = tf.cast(corrects,tf.float32)
        accrancy = tf.reduce_mean(corrects)

    with tf.name_scope('AdamOptimizer'):
        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_enropy)

    sotck_size = 60
    start = 0
    #stock_pack = []
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for epoch in range(50):
            for stocks in range(20):
                end = start + sotck_size
                stock_image = train[start:end]
                stock_label = label[start:end]

                start += sotck_size
                if start == 1200:
                    start = 0
                end += sotck_size
                ## train
                train_step.run(feed_dict={x: stock_image, y_: stock_label})

    eval_input = []
    eval_label = []
    for i in range(1200,1600):
        eval_input.apppend(train[i])
        eval_label.append(label[i])
    final_accuracy = accrancy.eval(feed_dict={x:eval_input,y_:eval_label})
    return final_accuracy

if __name__ == '__main__':
    final_accuracy = main()
    print final_accuracy


